---
permalink: /
title: "About Me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---



## Research Interest and Experience
#### [ML] ML-based automation, RLs, GA-based optimization, Transformer, Efficient attention for long sequence, Pruning, Quantization, Neural architecture search
#### [Accelerator] DNN accelerator, DNN Mapping/Dataflow, Algorithm-HW co-design



##  Projects
#### DNN Accelerator Design Space Exploration (DSE), GaTech, GA
•	Framed HW DSE into an RL problem and developed a REINFORCE-based algorithm
•	The proposed method outperforms 3 widely used optimization methods (SA, GA, BO) and 6 SOTA RLs
•	Main host of a popular open-sourced DNN accelerator cost model, [MAESTRO](http://maestro.ece.gatech.edu/), and a HW DSE framework, [ConfuciuX](https://github.com/maestro-project/confuciux)
#### DNN Dataflow and Schedule Optimization, GaTech, GA
•	The proposed genetic algorithm (GA)-based DNN dataflow mapper outperforms other optimizers by 10x-100x
•	Developed and hosted an open-sourced DNN dataflow mapper ([GAMMA](https://github.com/maestro-project/gamma)) supporting two most popular open-sourced DNN accelerator cost models MAESTRO and Timeloop
•	Developed a scheduler for multi-tenant DNN workload for multi-core accelerator, MAGMA
•	Developed a generalizable Decision-Transformer-based DNN mapper, DNNFuser
#### Efficient Attention and Transformers, GaTech, GA
•	Model pruning and efficient attention techniques for training large BERT model for long sequence tasks
•	Developed new dataflow tackling the quadratic memory bottleneck of attention layers
#### SW/HW Co-design for DNN Accelerator and Machine Learnings, GaTech, GA
•	Developed an optimized neural-evolution platform with algorithm-HW co-design approach and implemented on FPGA board (PYNQ)
•	Developed a GAN-based HW-aware Neural Architecture Search technique
•	Developed three RL-based methods optimizing Network-on-Chip performance




## Work Experiences
### Intern, Google, CA 05/21' - 08/21'
•	Research project on speed quality trade-off of efficient Transformer model
### Intern, Corporation Technology, Siemens, NJ										   05/19’ – 07/19’
•	Developed Pytorch DNN, RNN quantization module, for fast evaluation of any quantized DNN models

## Education
### Georgia Institute of Technology, Atlanta GA, Ph.D. in Electrical and Computer Engineering in Dr. Tushar Krishna's group - [Synergy Lab](https://synergy.ece.gatech.edu/), GPA: 3.75/4.0
**Majoring**: Computer Architecture & Software											   Expt: May 2022
**Selected Coursework**: Machine Learning Hardware Accelerator, Advanced Computer Architecture, High Performance Parallel Computing, Advanced Machine Learning, Interconnection Network, Digital Image Processing
### National Taiwan University, Taipei Taiwan, B.S., M.S. in Electronics Engineering, in Dr. An-Yeu Wu's group - [Access Lab](http://access.ee.ntu.edu.tw/), GPA: 3.95/4.0
**Selected Coursework**: Convex Optimization, GPU Programming, Embedded System



## Skills
#### Proficient: Python, Pytorch, JAX, GCP, Cloud TPU, Verilog
#### Experienced: Tensorflow, C/C++, Matlab
